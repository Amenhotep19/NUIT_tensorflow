{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4: Advanced Techniques and Convolutional Neural Nets",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/allemanau/NUIT_tensorflow/blob/master/notebooks/4_Advanced_Techniques_and_Convolutional_Neural_Nets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dug7p5MIpi5",
        "colab_type": "text"
      },
      "source": [
        "__4: Advanced Techniques and Convolutional Neural Nets__\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "In this notebook, we'll learn...\n",
        "- what a convolutional layer is, and why they work well for image data;\n",
        "- how to specify and fit a convnet in TensorFlow via the Keras API\n",
        "- use of data augmentation in image analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpXiKZW1pyEt",
        "colab_type": "text"
      },
      "source": [
        "Although our previous model seems to work pretty well, it regards pixel values as independent features, since we flattened the images. In reality, pixels are spatially related, and by flattening the data, we ignore more complex spatial information beyond pixel-to-pixel relationships that may improve predictive power."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7w2tPxC4KBW",
        "colab_type": "text"
      },
      "source": [
        "### Using a GPU runtime\n",
        "\n",
        "Convolutional nets are more expensive to train than fully connected neural nets. We will use GPU acceleration for this notebook to speed the process along. GPUs are in favor with machine learning enthusiasts because they are quick to process tensors and convolve images (this is, after all, a GPU's full-time job)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jw7I5CquQ9jY",
        "colab_type": "text"
      },
      "source": [
        "## __What are Convolutional Neural Nets?__\n",
        "\n",
        "The __convolutional neural network__ (or \"convnet\", or \"CNN\") uses alternating convolutional and max pooling layers to extract abstract features from image data.\n",
        "\n",
        "### __Convolutional layer__\n",
        "\n",
        "The workhorse in a CNN is the __convolutional layer__. The convolutional layer is composed of several __convolution filters__ (sometimes called \"kernels\"), which scan an image for certain shapes or features. A simplified example below illustrates how this works. When the \"eye kernel\" passes over an eye in the image, it activates, producing a large feature value:\n",
        "\n",
        "![](https://i.stack.imgur.com/9bi5k.gif)\n",
        "\n",
        "The kernel itself is a small matrix of weights, just a few pixels wide by a few pixels tall, and for color images, it has a depth equal to the number of color channels. It may seem from the example that kernels are complex, and thus need to be specified, but the weights for each kernel in the layer are actually learned by backpropagation.\n",
        "\n",
        "### Convolution operation\n",
        "\n",
        "Consider a  $3\\times3$ filter for an image:\n",
        "\n",
        "<img src=\"https://github.com/allemanau/NUIT_tensorflow/blob/master/images/conv_kernel.png?raw=1\">\n",
        "\n",
        "To get the convolution output, slide the filter over the image in each spot it fits, and take the weighted sum of the pixel values.\n",
        "\n",
        "<br>\n",
        "\n",
        "![](https://miro.medium.com/max/1070/1*Zx-ZMLKab7VOCQTxdZ1OAw.gif)\n",
        "\n",
        "<br>\n",
        "\n",
        "The convolutional output is a tensor of size\n",
        "\n",
        "<br>\n",
        "\\begin{equation*}\n",
        "\\text{(image width - filter width + 1)} \\times \\text{(image height - filter height + 1)} \\times (\\text{number of channels})\n",
        "\\end{equation*}\n",
        "<br>\n",
        "\n",
        "A layer is a collection of filters, each learning different features of the training images. Then, the size of the resulting convolutional layer will be\n",
        "\n",
        "<br>\n",
        "\\begin{equation*}\n",
        "\\text{(image width - filter width + 1)} \\times \\text{(image height - filter height + 1)} \\times (\\text{number of channels} = 1) \\times (\\text{number of filters})\n",
        "\\end{equation*}\n",
        "<br>\n",
        "\n",
        "Organizing convolutional layers is something of an art. In general, the key thing to remember is that __larger filter sizes extract generic global features__, while __smaller filter sizes extract complex local features__. Using smaller filter sizes also enables construction of a deeper network, potentially yielding performance gains at the cost of increased training and prediction time.\n",
        "\n",
        "### __Pooling__\n",
        "\n",
        "One problem with convolutional filters is that they activate precisely where the feature appears. A __pooling layer__ solves this problem by dividing a convolution filter into __patches__ and summarizing the content of the filter in that region. An extrapolation of the simple example above helps to illustrate how a pooling layer captures the same signature from a shifted version of the image:\n",
        "\n",
        "<br>\n",
        "\n",
        "![](https://i0.wp.com/www.thushv.com/wp-content/uploads/2018/05/pooling.gif?resize=1100%2C829)\n",
        "\n",
        "<br>\n",
        "\n",
        "This particular variant of pooling is called __max pooling__; it takes the largest activation value from the convolution patch to be the feature value in the pooling layer.\n",
        "\n",
        "### __Stacking convolutional and pooling layers to learn high level features__\n",
        "\n",
        "With the two primary tools of a CNN in hand, the general principle is to apply a convolutional layer, then a pooling layer, then another convolutional layer, and so on. We will do this for the MNIST data set in a moment.\n",
        "\n",
        "For novel problems, you can treat the layer sizes as parameters and tune them using a validation or cross-validation approach. Alternatively, you can adapt successful architectures others have built, [such as AlexNet](https://towardsdatascience.com/alexnet-the-architecture-that-challenged-cnns-e406d5297951), or even fine-tune the existing model for your problem via transfer learning (see the take-home exercise for more on this).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkwYwPEiRGS6",
        "colab_type": "text"
      },
      "source": [
        "Although there are a few aspects and tricks of convolutional layers left unexplained, we have enough in hand to apply 2D convolutional and max pooling layers to MNIST."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHCKtMQRRG85",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "5b93bcea-a637-40be-d2f6-dd15db9c7c8d"
      },
      "source": [
        "# Load libraries\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout, ActivityRegularization, Input, Reshape, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.utils import plot_model, to_categorical\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# Load MNIST data\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, raw_y_train), (x_test, raw_y_test) = mnist.load_data()\n",
        "x_train = x_train.reshape((x_train.shape[0], 28, 28, 1))\n",
        "x_test = x_test.reshape((x_test.shape[0], 28, 28, 1))\n",
        "\n",
        "x_train_scl = x_train / 255.0\n",
        "x_test_scl = x_test / 255.0\n",
        "\n",
        "y_train = to_categorical(raw_y_train, num_classes = 10)\n",
        "y_test = to_categorical(raw_y_test, num_classes = 10)\n",
        "\n",
        "loss_fn = 'categorical_crossentropy'\n",
        "optim = 'adam'\n",
        "batch_size = 500\n",
        "\n",
        "conv_model = Sequential()\n",
        "conv_model.add(Reshape((28, 28, 1)))\n",
        "conv_model.add(Conv2D(32, kernel_size=(3, 3), input_shape=(28,28)))\n",
        "conv_model.add(Conv2D(64, (3, 3)))\n",
        "conv_model.add(Activation('relu'))\n",
        "conv_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "conv_model.add(Dropout(0.2))\n",
        "conv_model.add(Flatten())\n",
        "conv_model.add(Dense(128))\n",
        "conv_model.add(Activation('relu'))\n",
        "conv_model.add(Dropout(0.5))\n",
        "conv_model.add(Dense(10))\n",
        "conv_model.add(Activation('softmax'))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thpjieS8RMqd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv_model.compile(optimizer = optim,\n",
        "              loss = loss_fn,\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', \n",
        "                                            patience=5, \n",
        "                                            restore_best_weights = True)\n",
        "\n",
        "conv_model.fit(x_train_scl, \n",
        "          y_train, \n",
        "          #batch_size = 100,\n",
        "          epochs = 50,\n",
        "          validation_split = 0.1,\n",
        "          callbacks = [callback])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pARa4gW7RWG-",
        "colab_type": "text"
      },
      "source": [
        "Our convolutional model, along with the other improvements made along the way, demonstrates superior performance -- ~99% accurate! For reference, the best kernel on Kaggle for MNIST is 99.75% accurate (and is quite a bit more complicated)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_zVGNzdRW0L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv_model.evaluate(x_test_scl,\n",
        "               y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmtGtbheRd69",
        "colab_type": "text"
      },
      "source": [
        "Again, let's peek at a selection of mistakes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKC3VHsqRkH0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get predicted classes.\n",
        "preds = conv_model.predict_classes(x_test_scl)\n",
        "\n",
        "# Prepare a figure for subplotting.\n",
        "fig = plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Select one random misclass for each digit in the data set.\n",
        "for i in range(10):\n",
        "    ind = np.random.choice(np.ndarray.flatten(np.argwhere((y_test == i) & (y_test != preds))))\n",
        "    actual = y_test[ind]\n",
        "    predicted = preds[ind]\n",
        "    plottable_image = np.reshape(x_test[ind], (28, 28))\n",
        "    ax = fig.add_subplot(2, 5, i+1)\n",
        "    ax.imshow(plottable_image, cmap='gray_r')\n",
        "    plt.xlabel(\"Actual: \" + \"{}\".format(actual) + \", Predicted: \" + \"{}\".format(predicted))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTUApur7mHqV",
        "colab_type": "text"
      },
      "source": [
        "## __Data Augmentation__\n",
        "\n",
        "An extremely useful trick to boost performance is to apply random __label-preserving transformations__ to create new training examples. These transformations include shifting and stretching images, adding small perturbations to the pixel values, and so on. This practice, called __data augmentation__, improves the ability of the model to generalize to unseen examples exhibiting these small changes. Keras provides an easy-to-use data augmentation toolkit. Let's explore it a bit on a single image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWyjpvW2UxUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOXRCZg1cqDC",
        "colab_type": "text"
      },
      "source": [
        "The `ImageDataGenerator` object will ultimately replace our training set. The object uses a training set as a source to generate batches of new observations from dynamically. The list of label-preserving transformations are\n",
        "\n",
        "- rotation\n",
        "- positional shift\n",
        "- brightness\n",
        "- shear\n",
        "- zoom\n",
        "- vertical/horizontal flip\n",
        "- others, see [`ImageDataGenerator` docs](https://keras.io/preprocessing/image/#imagedatagenerator-class)\n",
        "\n",
        "Let's try a simple generator that only shifts observations:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv9a6laXcqMD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Can specify as a fraction of total width. Let's set to 30% to demonstrate:\n",
        "train_gen = ImageDataGenerator(\n",
        "    width_shift_range=0.3,\n",
        "    height_shift_range=0.3)\n",
        "\n",
        "for x_batch, y_batch in train_gen.flow(x_train, y_train, batch_size=9):\n",
        "\tfor i in range(0, 9):\n",
        "\t\tplt.subplot(330 + 1 + i)\n",
        "\t\tplt.imshow(x_batch[i].reshape(28, 28), cmap = plt.get_cmap('gray'))\n",
        "\tplt.show()\n",
        "\tbreak"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDdnSC9VfeVO",
        "colab_type": "text"
      },
      "source": [
        "We can add rotation and shear if so desired:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtsmLG2tfeet",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Rotation range is in degrees. Shear range is \n",
        "train_gen = ImageDataGenerator(\n",
        "    rotation_range = 20,\n",
        "    shear_range = 30,\n",
        "    width_shift_range=0.15,\n",
        "    height_shift_range=0.15)\n",
        "\n",
        "for x_batch, y_batch in train_gen.flow(x_train, y_train, batch_size=9):\n",
        "\tfor i in range(0, 9):\n",
        "\t\tplt.subplot(330 + 1 + i)\n",
        "\t\tplt.imshow(x_batch[i].reshape(28, 28), cmap = plt.get_cmap('gray'))\n",
        "\tplt.show()\n",
        "\tbreak"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlAYzr2ZhN09",
        "colab_type": "text"
      },
      "source": [
        "Even though flips don't make sense for our data, we could add them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zcjDknUhSMi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "e03e9cd1-9a80-4dec-9ce6-80540cc5c7a4"
      },
      "source": [
        "# Rotation range is in degrees. Shear range is \n",
        "train_gen = ImageDataGenerator(\n",
        "    rotation_range = 20,\n",
        "    shear_range = 30,\n",
        "    width_shift_range=0.15,\n",
        "    height_shift_range=0.15,\n",
        "    horizontal_flip = True,\n",
        "    vertical_flip = True)\n",
        "\n",
        "for x_batch, y_batch in train_gen.flow(x_train, y_train, batch_size=9):\n",
        "\tfor i in range(0, 9):\n",
        "\t\tplt.subplot(330 + 1 + i)\n",
        "\t\tplt.imshow(x_batch[i].reshape(28, 28), cmap = plt.get_cmap('gray'))\n",
        "\tplt.show()\n",
        "\tbreak"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU4AAAD7CAYAAAAFI30bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIyY2cRqGYcTEJk7DMIyY2MRpGIYRk/8HOf35qvBBOrwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 9 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNCt_JDcwke1",
        "colab_type": "text"
      },
      "source": [
        "Let's set up a generator to transform the training set, and train a model using a random continuous `flow()` of these examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsVPibFEkGz8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "42b17980-bb94-4246-ade3-a19ec69a3589"
      },
      "source": [
        "# Set up a generator. Params:\n",
        "# Random rotation between +/- 8 degrees\n",
        "# Random translations of +/- 8% of total width\n",
        "# Random zooms of +/- 8% of total size.\n",
        "# Random shear of +/- 15 degrees\n",
        "train_gen = ImageDataGenerator(rotation_range = 8, \n",
        "                               width_shift_range = 0.08, \n",
        "                               shear_range = 15, \n",
        "                               height_shift_range = 0.08, \n",
        "                               zoom_range = 0.08 )\n",
        "\n",
        "# Set up a flow from our generator.\n",
        "train_set = train_gen.flow(x_train_scl, y_train, batch_size = batch_size)\n",
        "\n",
        "# Set up a second validation flow from the same generator.\n",
        "val_set = train_gen.flow(x_train_scl, y_train, batch_size=100)\n",
        "\n",
        "# Set up and compile model.\n",
        "conv_model = Sequential()\n",
        "conv_model.add(Reshape((28, 28, 1)))\n",
        "conv_model.add(Conv2D(32, kernel_size=(3, 3), input_shape=(28,28)))\n",
        "conv_model.add(Conv2D(64, (3, 3)))\n",
        "conv_model.add(Activation('relu'))\n",
        "conv_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "conv_model.add(Dropout(0.2))\n",
        "conv_model.add(Flatten())\n",
        "conv_model.add(Dense(128))\n",
        "conv_model.add(Activation('relu'))\n",
        "conv_model.add(Dropout(0.5))\n",
        "conv_model.add(Dense(10))\n",
        "conv_model.add(Activation('softmax'))\n",
        "\n",
        "conv_model.compile(optimizer = optim,\n",
        "              loss = loss_fn,\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "# Validation steps argument only applies when ImageDataGenerator is used.\n",
        "conv_model.fit(train_set, \n",
        "                         steps_per_epoch = 60000//batch_size, \n",
        "                         validation_data = val_set, \n",
        "                         validation_steps = 10000//batch_size, \n",
        "                         epochs = 40)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "Train for 120 steps, validate for 20 steps\n",
            "Epoch 1/40\n",
            "120/120 [==============================] - 15s 128ms/step - loss: 0.6093 - accuracy: 0.8024 - val_loss: 0.1828 - val_accuracy: 0.9485\n",
            "Epoch 2/40\n",
            "120/120 [==============================] - 15s 127ms/step - loss: 0.2328 - accuracy: 0.9285 - val_loss: 0.1101 - val_accuracy: 0.9645\n",
            "Epoch 3/40\n",
            "120/120 [==============================] - 15s 125ms/step - loss: 0.1807 - accuracy: 0.9456 - val_loss: 0.0777 - val_accuracy: 0.9780\n",
            "Epoch 4/40\n",
            "120/120 [==============================] - 15s 129ms/step - loss: 0.1515 - accuracy: 0.9542 - val_loss: 0.0709 - val_accuracy: 0.9750\n",
            "Epoch 5/40\n",
            "120/120 [==============================] - 15s 124ms/step - loss: 0.1411 - accuracy: 0.9574 - val_loss: 0.0636 - val_accuracy: 0.9800\n",
            "Epoch 6/40\n",
            "120/120 [==============================] - 15s 128ms/step - loss: 0.1320 - accuracy: 0.9600 - val_loss: 0.0507 - val_accuracy: 0.9835\n",
            "Epoch 7/40\n",
            "120/120 [==============================] - 15s 126ms/step - loss: 0.1202 - accuracy: 0.9637 - val_loss: 0.0511 - val_accuracy: 0.9850\n",
            "Epoch 8/40\n",
            "120/120 [==============================] - 15s 129ms/step - loss: 0.1095 - accuracy: 0.9668 - val_loss: 0.0482 - val_accuracy: 0.9835\n",
            "Epoch 9/40\n",
            "120/120 [==============================] - 15s 124ms/step - loss: 0.1083 - accuracy: 0.9668 - val_loss: 0.0462 - val_accuracy: 0.9830\n",
            "Epoch 10/40\n",
            "120/120 [==============================] - 15s 126ms/step - loss: 0.1028 - accuracy: 0.9691 - val_loss: 0.0437 - val_accuracy: 0.9855\n",
            "Epoch 11/40\n",
            "120/120 [==============================] - 15s 126ms/step - loss: 0.1013 - accuracy: 0.9695 - val_loss: 0.0519 - val_accuracy: 0.9830\n",
            "Epoch 12/40\n",
            "120/120 [==============================] - 16s 132ms/step - loss: 0.0979 - accuracy: 0.9712 - val_loss: 0.0328 - val_accuracy: 0.9895\n",
            "Epoch 13/40\n",
            "120/120 [==============================] - 15s 124ms/step - loss: 0.0958 - accuracy: 0.9715 - val_loss: 0.0365 - val_accuracy: 0.9875\n",
            "Epoch 14/40\n",
            "120/120 [==============================] - 15s 127ms/step - loss: 0.0940 - accuracy: 0.9721 - val_loss: 0.0416 - val_accuracy: 0.9865\n",
            "Epoch 15/40\n",
            "120/120 [==============================] - 15s 125ms/step - loss: 0.0883 - accuracy: 0.9736 - val_loss: 0.0267 - val_accuracy: 0.9900\n",
            "Epoch 16/40\n",
            "120/120 [==============================] - 15s 127ms/step - loss: 0.0901 - accuracy: 0.9726 - val_loss: 0.0329 - val_accuracy: 0.9915\n",
            "Epoch 17/40\n",
            "120/120 [==============================] - 15s 128ms/step - loss: 0.0887 - accuracy: 0.9735 - val_loss: 0.0314 - val_accuracy: 0.9880\n",
            "Epoch 18/40\n",
            "120/120 [==============================] - 15s 125ms/step - loss: 0.0815 - accuracy: 0.9752 - val_loss: 0.0323 - val_accuracy: 0.9890\n",
            "Epoch 19/40\n",
            "120/120 [==============================] - 15s 127ms/step - loss: 0.0819 - accuracy: 0.9750 - val_loss: 0.0250 - val_accuracy: 0.9900\n",
            "Epoch 20/40\n",
            "120/120 [==============================] - 15s 126ms/step - loss: 0.0798 - accuracy: 0.9759 - val_loss: 0.0352 - val_accuracy: 0.9875\n",
            "Epoch 21/40\n",
            "120/120 [==============================] - 15s 125ms/step - loss: 0.0785 - accuracy: 0.9765 - val_loss: 0.0301 - val_accuracy: 0.9885\n",
            "Epoch 22/40\n",
            "120/120 [==============================] - 15s 126ms/step - loss: 0.0799 - accuracy: 0.9763 - val_loss: 0.0297 - val_accuracy: 0.9880\n",
            "Epoch 23/40\n",
            "120/120 [==============================] - 15s 128ms/step - loss: 0.0748 - accuracy: 0.9777 - val_loss: 0.0273 - val_accuracy: 0.9935\n",
            "Epoch 24/40\n",
            "120/120 [==============================] - 15s 127ms/step - loss: 0.0726 - accuracy: 0.9781 - val_loss: 0.0215 - val_accuracy: 0.9940\n",
            "Epoch 25/40\n",
            "120/120 [==============================] - 15s 125ms/step - loss: 0.0786 - accuracy: 0.9761 - val_loss: 0.0269 - val_accuracy: 0.9905\n",
            "Epoch 26/40\n",
            "120/120 [==============================] - 15s 125ms/step - loss: 0.0743 - accuracy: 0.9777 - val_loss: 0.0202 - val_accuracy: 0.9930\n",
            "Epoch 27/40\n",
            "120/120 [==============================] - 15s 126ms/step - loss: 0.0722 - accuracy: 0.9782 - val_loss: 0.0248 - val_accuracy: 0.9920\n",
            "Epoch 28/40\n",
            "120/120 [==============================] - 15s 129ms/step - loss: 0.0722 - accuracy: 0.9789 - val_loss: 0.0290 - val_accuracy: 0.9910\n",
            "Epoch 29/40\n",
            "120/120 [==============================] - 15s 124ms/step - loss: 0.0681 - accuracy: 0.9794 - val_loss: 0.0282 - val_accuracy: 0.9905\n",
            "Epoch 30/40\n",
            "120/120 [==============================] - 15s 124ms/step - loss: 0.0663 - accuracy: 0.9803 - val_loss: 0.0229 - val_accuracy: 0.9915\n",
            "Epoch 31/40\n",
            "120/120 [==============================] - 15s 128ms/step - loss: 0.0671 - accuracy: 0.9794 - val_loss: 0.0262 - val_accuracy: 0.9910\n",
            "Epoch 32/40\n",
            "120/120 [==============================] - 15s 128ms/step - loss: 0.0653 - accuracy: 0.9806 - val_loss: 0.0249 - val_accuracy: 0.9915\n",
            "Epoch 33/40\n",
            "120/120 [==============================] - 15s 125ms/step - loss: 0.0677 - accuracy: 0.9800 - val_loss: 0.0234 - val_accuracy: 0.9940\n",
            "Epoch 34/40\n",
            "120/120 [==============================] - 15s 126ms/step - loss: 0.0655 - accuracy: 0.9801 - val_loss: 0.0179 - val_accuracy: 0.9950\n",
            "Epoch 35/40\n",
            "120/120 [==============================] - 15s 128ms/step - loss: 0.0640 - accuracy: 0.9809 - val_loss: 0.0233 - val_accuracy: 0.9905\n",
            "Epoch 36/40\n",
            "120/120 [==============================] - 15s 126ms/step - loss: 0.0621 - accuracy: 0.9811 - val_loss: 0.0207 - val_accuracy: 0.9925\n",
            "Epoch 37/40\n",
            "120/120 [==============================] - 15s 128ms/step - loss: 0.0617 - accuracy: 0.9815 - val_loss: 0.0256 - val_accuracy: 0.9925\n",
            "Epoch 38/40\n",
            "120/120 [==============================] - 15s 129ms/step - loss: 0.0629 - accuracy: 0.9807 - val_loss: 0.0184 - val_accuracy: 0.9930\n",
            "Epoch 39/40\n",
            "120/120 [==============================] - 15s 126ms/step - loss: 0.0615 - accuracy: 0.9811 - val_loss: 0.0269 - val_accuracy: 0.9910\n",
            "Epoch 40/40\n",
            "120/120 [==============================] - 15s 126ms/step - loss: 0.0599 - accuracy: 0.9813 - val_loss: 0.0223 - val_accuracy: 0.9920\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f93c053b8d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJgquKHqxZzS",
        "colab_type": "text"
      },
      "source": [
        "Evaluation should yield accuracy of around 99.4%, about 0.4% off the best known classifier. Not too shabby! To boost this much higher, we'd need to complicate the model architecture. I encourage you to check out examples on Kaggle; for instance, [this one hits 99.7% accuracy](https://www.kaggle.com/yassineghouzam/introduction-to-cnn-keras-0-997-top-6)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CylwaXSpnps8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7f4c27a5-2a16-4403-c5da-8874b5e2330e"
      },
      "source": [
        "conv_model.evaluate(x_test_scl, y_test)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 94us/sample - loss: 0.0199 - accuracy: 0.9936\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.019921857891002356, 0.9936]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    }
  ]
}